# -*- coding: utf-8 -*-
"""assgn9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1np-zriCUZEFTQyBWRnm3Zb4LHOX3E2JD
"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer
from nltk.probability import FreqDist
import re
import matplotlib.pyplot as plt

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

input_text = """
DC's legendary icons, Batman and Superman, return together in a highly awaited cinematic event.
Ben Affleck's brooding Dark Knight teams up with Henry Cavill's heroic Man of Steel.
Fans can expect gripping action, moral dilemmas, and iconic visuals.
With elements of the multiverse and appearances from Justice League members,
this film pays homage to decades of DC storytelling.
It may very well reshape the landscape of superhero cinema.
"""

print("=== Task 1: Basic Text Preprocessing ===")

# Step 1: Lowercase and remove punctuation
text_lower = input_text.lower()
text_no_punct = re.sub(r'[^\w\s]', '', text_lower)
print("\n1. Cleaned Text:\n", text_no_punct)

# Step 2: Tokenization
word_list = word_tokenize(text_no_punct)
sentence_list = sent_tokenize(input_text)
print("\n2. Word Tokens:", word_list[:10], "...")
print("   Sentence Tokens:", sentence_list[:2])

# Step 3: Stopword Removal
stop_words = set(stopwords.words('english'))
important_words = [w for w in word_list if w not in stop_words]
print("\n3. After Removing Stopwords:", important_words[:10], "...")

# Step 4: Word Frequency Distribution
freq_dist = FreqDist(important_words)
print("\n4. Frequency Distribution:")
print(freq_dist)
freq_dist.plot(10, title="Top Frequent Words")

# Task 2: Stemming and Lemmatization
print("\n=== Task 2: Stemming and Lemmatization ===")

porter_stem = PorterStemmer()
lancaster_stem = LancasterStemmer()
lemmatize = WordNetLemmatizer()

porter_result = [porter_stem.stem(word) for word in important_words]
lancaster_result = [lancaster_stem.stem(word) for word in important_words]
lemmatized_result = [lemmatize.lemmatize(word) for word in important_words]

print("\nOriginal:", important_words[:10])
print("Porter Stemmer:", porter_result[:10])
print("Lancaster Stemmer:", lancaster_result[:10])
print("Lemmatized:", lemmatized_result[:10])

# Task 3: Regex Operations
print("\n=== Task 3: Regex Matching ===")

long_word_matches = re.findall(r'\b\w{6,}\b', text_no_punct)
print("\na. Words >5 Characters:", long_word_matches)

digits_found = re.findall(r'\d+', text_no_punct)
print("b. Numbers Found:", digits_found)

cap_words = re.findall(r'\b[A-Z][a-z]+\b', input_text)
print("c. Capitalized Words:", cap_words)

# More Splits
alpha_only = re.findall(r'\b[a-z]+\b', text_no_punct)
vowel_start = re.findall(r'\b[aeiou][a-z]*\b', text_no_punct)

print("\n3a. Alphabetic Words:", alpha_only[:10])
print("3b. Words Starting with Vowels:", vowel_start)

# Task 4: Custom Tokenizer and Regex Cleaning
print("\n=== Task 4: Custom Tokenization & Cleaning ===")

def smart_tokenizer(sample_text):
    regex = r"""
        \b\w+(?:'\w+)?\b            # words with contractions
        | \b\d+\.\d+\b              # decimal numbers
        | \b\d+\b                   # integers
        | \b\w+(?:-\w+)+\b          # hyphenated words
    """
    return re.findall(regex, sample_text, re.VERBOSE)

demo_text = input_text + " For queries, contact esingla_be23@thapar.edu or visit https://eshansingla.com. Call +91 9876543210."

tokens_custom = smart_tokenizer(demo_text.lower())
print("\n2. Custom Tokens:", tokens_custom)

# Regex Substitution
text_scrubbed = re.sub(r'\S+@\S+', '<EMAIL>', demo_text)
text_scrubbed = re.sub(r'https?://\S+', '<URL>', text_scrubbed)
text_scrubbed = re.sub(r'(\+\d{1,3} \d{10}|\d{3}-\d{3}-\d{4})', '<PHONE>', text_scrubbed)

print("\n3. Cleaned Text After Substitutions:\n", text_scrubbed)