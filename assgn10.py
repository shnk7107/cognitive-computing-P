# -*- coding: utf-8 -*-
"""assgn10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2syULCOkZppmeeW1f1hQ4K0lE-caKts
"""

import os
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from textblob import TextBlob
from wordcloud import WordCloud
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
import matplotlib.pyplot as plt

os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
nltk.download('all')

# --- Q1: Text Processing ---
text_q1 = "DC superheroes like Batman, Superman, and Wonder Woman are incredible. They inspire us with their bravery and strength. DC movies blend action, intensity, and heroism in perfect balance. These stories remind us to be just and fearless. Watching the Justice League unite is always exciting, and their battles keep us captivated."

text_lower = text_q1.lower()
text_cleaned = re.sub(r'[^\w\s]', '', text_lower)

sentences = sent_tokenize(text_cleaned)
words_nltk = word_tokenize(text_cleaned)
words_split = text_cleaned.split()

stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words_nltk if word not in stop_words]

fdist = nltk.FreqDist(filtered_words)

words_only = re.findall(r'\b[a-zA-Z]+\b', text_cleaned)
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stemmed_words = [stemmer.stem(word) for word in filtered_words]
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]

# --- Q3: Text Representation (BoW, TF-IDF) ---
texts_q3 = [
    "Batmanâ€™s gadgets are powered by cutting-edge technology.",
    "Superman's strength and flight make him nearly invincible.",
    "Wonder Woman wields the Lasso of Truth and fights for justice."
]

cv = CountVectorizer()
bow_matrix = cv.fit_transform(texts_q3)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(texts_q3)
keywords = [tfidf.get_feature_names_out()[idx] for idx in tfidf_matrix.toarray().argsort(axis=1)[:, -3:]]

# --- Q4: Similarity Measures ---
text1 = "Batman and Superman are iconic DC heroes."
text2 = "Superman's powers and Batman's intellect make them legendary."
words1 = set(word_tokenize(re.sub(r'[^\w\s]', '', text1.lower())))
words2 = set(word_tokenize(re.sub(r'[^\w\s]', '', text2.lower())))
jaccard_similarity = len(words1 & words2) / len(words1 | words2)

texts_tfidf = TfidfVectorizer()
vectors = texts_tfidf.fit_transform([text1, text2])
cosine_sim = cosine_similarity(vectors[0:1], vectors[1:2])

# --- Q5: Sentiment Analysis ---
review = "The Justice League movie was absolutely incredible."
blob = TextBlob(review)
polarity = blob.sentiment.polarity
subjectivity = blob.sentiment.subjectivity

wc = WordCloud().generate(review)
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

# --- Q6: Tokenization and LSTM Sequence Model ---
text_train = "Batman is a strategist who uses technology and intellect to protect Gotham."
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text_train])
sequences = []
words = text_train.split()

for i in range(1, len(words)):
    seq = words[:i + 1]
    tokenized_seq = tokenizer.texts_to_sequences([' '.join(seq)])[0]
    sequences.append(tokenized_seq)

padded = pad_sequences(sequences)

model = Sequential()
model.add(Embedding(input_dim=50, output_dim=10, input_length=padded.shape[1]))
model.add(LSTM(50))
model.add(Dense(50, activation='relu'))
model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
model.summary()